#####################################
# Task 1.1: Playing Around With The LLM
# This script creates a simple prompt to ask about the sky's color
# using the OLMo language model
#####################################

# Import necessary libraries from transformers for working with the LLM
from transformers import AutoModelForCausalLM  # For loading the language model
from transformers import AutoTokenizer         # For tokenizing input text

# Define the model identifier
# We're using AMD's OLMo model, which is a 1B parameter model
model_id = 'amd/AMD-OLMo-1B-SFT-DPO'

# Initialize the tokenizer
# This converts our text into tokens that the model can understand
print("\nLoading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Initialize the model and move it to GPU
# device_map="cuda:0" ensures the model uses the GPU for faster processing
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cuda:0"  # Specify GPU usage
)

# Create the chat prompt using the required format
# The prompt is a list of dictionaries with 'role' and 'content' keys
print("\nCreating chat prompt...")
chat = [
    {
        "role": "user",           # Specify that this is user input
        "content": "What is the color of the sky?"  # The actual question
    }
]

# Apply the chat template to format the prompt for the model
# This converts our chat format into tokens the model can process
print("Tokenizing chat...")
tokenized_chat = tokenizer.apply_chat_template(
    chat,                    # Our chat prompt
    add_generation_prompt=True,   # Add necessary generation tokens
    max_length=128,              # Maximum length of the input
    return_tensors="pt"          # Return PyTorch tensors
).to('cuda')                     # Move to GPU

# Print the decoded tokenized chat to verify the format
# This shows us how the model sees our input
print("\nDecoded tokenized chat (Model's input format):")
print(tokenizer.decode(tokenized_chat[0]))

# Generate the model's response
print("\nGenerating response...")
outputs = model.generate(
    tokenized_chat,              # Our tokenized input
    max_new_tokens=128,          # Maximum length of the response
    pad_token_id=tokenizer.eos_token_id  # Use end of sequence token for padding
)

# Decode and print the complete response
# This converts the model's token outputs back into readable text
print("\nModel's complete response:")
print(tokenizer.decode(outputs[0]))

# Additional information about the process
print("\nProcess complete!")
print("- Input tokens shape:", tokenized_chat.shape)
print("- Output tokens shape:", outputs.shape)

#####################################
# Usage Notes:
# 1. Ensure you're using a GPU runtime in Colab
# 2. The model may take a moment to load
# 3. Response generation time depends on max_new_tokens
#####################################





#####################################
# Task 1.2: Generating Output with the LLM
# This script generates and decodes output from the OLMo model
# for the sky color question
#####################################

# Import necessary libraries
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch  # For handling tensor operations

# Initialize model and tokenizer
print("\nInitializing model and tokenizer...")
model_id = 'amd/AMD-OLMo-1B-SFT-DPO'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")

# Create the chat prompt
print("\nCreating chat prompt...")
chat = [{"role": "user", "content": "What is the color of the sky?"}]

# Tokenize the chat and move to GPU
print("\nTokenizing chat and moving to GPU...")
tokenized_chat = tokenizer.apply_chat_template(
    chat,
    add_generation_prompt=True,
    max_length=128,
    return_tensors="pt"
).to('cuda')

# Generate output with specified parameters
print("\nGenerating model response...")
with torch.no_grad():  # Disable gradient calculation for efficiency
    outputs = model.generate(
        tokenized_chat,
        max_new_tokens=256,  # Set maximum tokens to generate as specified
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,     # Enable sampling for more natural responses
        temperature=0.7,    # Control randomness (0.7 is a good balance)
        top_p=0.9          # Nucleus sampling parameter
    )

# Decode and print the complete conversation
print("\nComplete conversation:")
print("-" * 50)
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded_output)
print("-" * 50)

# Extract just the model's response (everything after the question)
print("\nModel's response only:")
print("-" * 50)
response_start = decoded_output.find("What is the color of the sky?") + len("What is the color of the sky?")
model_response = decoded_output[response_start:].strip()
print(model_response)
print("-" * 50)

# Print some statistics about the generation
print("\nGeneration Statistics:")
print(f"Input length: {len(tokenized_chat[0])} tokens")
print(f"Output length: {len(outputs[0])} tokens")
print(f"New tokens generated: {len(outputs[0]) - len(tokenized_chat[0])} tokens")






#####################################
# Task 1.3: Keeping Just the LLM's Response
# This script demonstrates how to extract only the model's response
# without the input prompt or special tokens
#####################################

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Initialize model and tokenizer
model_id = 'amd/AMD-OLMo-1B-SFT-DPO'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")

# Create chat and tokenize
chat = [{"role": "user", "content": "What is the color of the sky?"}]
tokenized_chat = tokenizer.apply_chat_template(
    chat,
    add_generation_prompt=True,
    max_length=128,
    return_tensors="pt"
).to('cuda')

# Generate response
with torch.no_grad():
    outputs = model.generate(
        tokenized_chat,
        max_new_tokens=256,
        pad_token_id=tokenizer.eos_token_id
    )

# Get the length of the original input as specified
input_length = tokenized_chat.shape[1]

# Get the generated tokens using exact format from instructions
generated_tokens = outputs[0][input_length:]

# Decode with skip_special_tokens=True as required
final_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)

# Print final response only, as specified
print("Model's response:")
print(final_response)




#####################################
# Task 1.4: Extending the Conversation
# Using previous LLM response to create a continued conversation
#####################################

# Import necessary libraries
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Initialize model and tokenizer
print("\nInitializing model and tokenizer...")
model_id = 'amd/AMD-OLMo-1B-SFT-DPO'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")

# Create the extended chat using the actual response from Task 1.3
chat = [
    {"role": "user", "content": "What is the color of the sky?"},
    {"role": "assistant", "content": "The color of the sky is typically blue. However, the color can vary depending on the time of day, weather conditions, and altitude. At dawn, the sky may appear to be a light gray or white, while at sunset, it may appear to be a deep orange or red. During the day, the sky is usually a blue color."},
    {"role": "user", "content": "Tell me more about that color."}
]

# Tokenize the extended chat
print("\nTokenizing extended chat...")
tokenized_chat = tokenizer.apply_chat_template(
    chat,
    add_generation_prompt=True,
    max_length=512,  # Increased due to longer conversation
    return_tensors="pt"
).to('cuda')

# Generate response for the follow-up question
print("\nGenerating response...")
with torch.no_grad():
    outputs = model.generate(
        tokenized_chat,
        max_new_tokens=256,
        pad_token_id=tokenizer.eos_token_id
    )

# Extract only the new response
input_length = tokenized_chat.shape[1]
generated_tokens = outputs[0][input_length:]
final_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)

# Print the LLM's response to the follow-up question
print("\nLLM's response to 'Tell me more about that color.':")
print("-" * 50)
print(final_response)
print("-" * 50)






#####################################
# Task 1.5: Using Pipeline for Text Generation
# This script demonstrates the use of Hugging Face pipeline
# for automated text generation
#####################################

# Import pipeline as shown in instructions
from transformers import pipeline

# Define model identifier
model_id = 'amd/AMD-OLMo-1B-SFT-DPO'

# Define pipeline exactly as shown in the task
pipe = pipeline("text-generation", model_id, device='cuda')

# Chat object from Task 1.4 (non-tokenized version)
chat = [
    {"role": "user", "content": "What is the color of the sky?"},
    {"role": "assistant", "content": "The color of the sky is typically blue. However, the color can vary depending on the time of day, weather conditions, and altitude. At dawn, the sky may appear to be a light gray or white, while at sunset, it may appear to be a deep orange or red. During the day, the sky is usually a blue color."},
    {"role": "user", "content": "Tell me more about that color."}
]

# Generate output in one line as specified
output = pipe(chat, max_new_tokens=256)

# Look at the output using the specified format
print("Pipeline Output:")
print("-" * 50)
print(output[0]['generated_text'])
print("-" * 50)

# Display output structure for analysis
print("\nOutput Structure Analysis:")
print(f"Output type: {type(output)}")
print(f"Number of entries: {len(output)}")
print(f"Keys in output[0]: {output[0].keys()}")

# Extract conversation history for better readability
generated_conversation = output[0]['generated_text']
print("\nFormatted Conversation:")
print("-" * 50)
for i, message in enumerate(generated_conversation):
    role = message['role'].title()
    content = message['content']
    print(f"{role}: {content}")
    print()
print("-" * 50)






#####################################
# Task 1.6 (Optional): Extracting the Outputs (2 Extra Credit Points)
# This script extracts only the LLM's last response from the conversation
#####################################

# Get the previous outputs from Task 1.5
previous_output = [
    {'role': 'user', 'content': 'What is the color of the sky?'},
    {'role': 'assistant', 'content': 'The color of the sky is typically blue. However, the color can vary depending on the time of day, weather conditions, and altitude. At dawn, the sky may appear to be a light gray or white, while at sunset, it may appear to be a deep orange or red. During the day, the sky is usually a blue color.'},
    {'role': 'user', 'content': 'Tell me more about that color.'},
    {'role': 'assistant', 'content': "The color of the sky during the day is primarily due to the way light interacts with the atmosphere. When sunlight enters the Earth's atmosphere, it is scattered in various ways, creating a range of colors. Blue light is scattered more than other colors, which is why the sky appears blue during the day.\n\nThe color of the sky can also change throughout the day due to factors such as atmospheric conditions, weather patterns, and the presence of clouds. For example, during a clear, sunny day, the sky may appear blue, while during a cloudy day, the sky may appear to be a different color, such as gray or white.\n\nThe color of the sky can also vary depending on the time of day. During the morning and evening, when the sun is low on the horizon, the sky appears to be a lighter shade of blue. This is because the sunlight is scattered more by the atmosphere, creating a more diffuse light that appears to be a lighter color.\n\nAt sunrise and sunset, the sky appears to be a deeper shade of blue, as the sunlight is scattered more by the atmosphere, creating a more diffuse light that appears to be a deeper color.\n\nThe color of the sky can also change due to the presence of clouds"}
]

# Extract just the last assistant response
last_response = [msg['content'] for msg in previous_output if msg['role'] == 'assistant'][-1]

# Print the extracted response
print("LLM's last response:")
print("-" * 50)
print(last_response)
print("-" * 50)

# Alternative method using list comprehension and indexing
def extract_last_assistant_response(conversation):
    """
    Extract the last assistant response from a conversation.
    
    Args:
        conversation (list): List of dictionaries with 'role' and 'content' keys
        
    Returns:
        str: The content of the last assistant message
    """
    assistant_responses = [msg['content'] for msg in conversation if msg['role'] == 'assistant']
    return assistant_responses[-1] if assistant_responses else ""

# Test the function
extracted_response = extract_last_assistant_response(previous_output)
print("\nUsing function extraction:")
print("-" * 50)
print(extracted_response)
print("-" * 50)

# Additional analysis
print("\nConversation Analysis:")
print(f"Total messages: {len(previous_output)}")
print(f"User messages: {len([msg for msg in previous_output if msg['role'] == 'user'])}")
print(f"Assistant messages: {len([msg for msg in previous_output if msg['role'] == 'assistant'])}")
print(f"Last response length: {len(last_response)} characters")




#####################################
# Task 1.7: Looking at the Data and Creating Prompts
# This script loads the affective polarization dataset and creates
# prompts for polarizing rhetoric detection
#####################################

import numpy as np
import pandas as pd

# Download the dataset (if not already downloaded)
# !gdown 1w1rNOGCfM4wNO1KASnvx6FBHwxiODOh_

# Load the affective polarization dataset
print("Loading affective polarization dataset...")
df = pd.read_csv('affective_polarization_train.csv')

# Display basic information about the dataset
print("\nDataset Information:")
print(f"Dataset shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
print("\nFirst few rows:")
print(df.head())

# Display data distribution
print("\nLabel distribution:")
print(df['labels'].value_counts())
print("\nParty distribution:")
print(df['party'].value_counts())

# Sample 50 tweets from the dataset
print("\nSampling 50 tweets...")
sampled_tweets = df.sample(n=50, random_state=42)

# Initialize empty list for storing prompts
tweets_prompts = []

# Create prompts with exact format specified in the task
print("Creating prompts...")
for _, row in sampled_tweets.iterrows():
    prompt = (
        "Does this tweet contain polarizing rhetoric?\n\n"
        f"Tweet: {row['text']}\n\n"
        "Answer:"
    )
    tweets_prompts.append(prompt)

# Display information about created prompts
print(f"\nTotal prompts created: {len(tweets_prompts)}")

# Print first 3 prompts as examples
print("\nFirst 3 prompts as examples:")
print("-" * 60)
for i in range(3):
    print(f"Prompt {i+1}:")
    print(tweets_prompts[i])
    print("-" * 60)

# Display some statistics
print("\nPrompt Statistics:")
prompt_lengths = [len(prompt) for prompt in tweets_prompts]
print(f"Average prompt length: {np.mean(prompt_lengths):.1f} characters")
print(f"Min prompt length: {min(prompt_lengths)} characters")
print(f"Max prompt length: {max(prompt_lengths)} characters")

# Show distribution of actual labels for the sampled tweets
print("\nSampled tweets label distribution:")
print(sampled_tweets['labels'].value_counts())
print("\nSampled tweets party distribution:")
print(sampled_tweets['party'].value_counts())

# Save the sampled data and prompts for later use
sampled_tweets.to_csv('sampled_tweets.csv', index=False)
print("\nSampled tweets saved to 'sampled_tweets.csv'")

# Save prompts to a text file
with open('tweets_prompts.txt', 'w', encoding='utf-8') as f:
    for i, prompt in enumerate(tweets_prompts):
        f.write(f"Prompt {i+1}:\n{prompt}\n{'='*60}\n")
print("Prompts saved to 'tweets_prompts.txt'")




#####################################
# Task 1.8: Generate responses for all 50 prompts
# This script processes all created prompts through the LLM pipeline
# and stores the responses for analysis
#####################################

from transformers import pipeline
import time
import json

# Create pipeline with GPU
print("Initializing pipeline...")
pipe = pipeline(
    "text-generation",
    "amd/AMD-OLMo-1B-SFT-DPO",
    device="cuda"
)

# Load the prompts created in Task 1.7
# (Assuming tweets_prompts is available from the previous task)
# If running separately, uncomment and run the data loading section:

"""
import pandas as pd
df = pd.read_csv('affective_polarization_train.csv')
sampled_tweets = df.sample(n=50, random_state=42)
tweets_prompts = []
for _, row in sampled_tweets.iterrows():
    prompt = (
        "Does this tweet contain polarizing rhetoric?\n\n"
        f"Tweet: {row['text']}\n\n"
        "Answer:"
    )
    tweets_prompts.append(prompt)
"""

# Initialize list to store responses
responses = []

print(f"Generating responses for {len(tweets_prompts)} prompts...")
print("This may take several minutes...")

# Track timing
start_time = time.time()

# Generate responses for each prompt
for i, prompt in enumerate(tweets_prompts):
    try:
        # Generate response using pipeline
        output = pipe(prompt, max_new_tokens=256)
        responses.append(output)
        
        # Print progress every 10 prompts
        if (i + 1) % 10 == 0:
            elapsed_time = time.time() - start_time
            avg_time = elapsed_time / (i + 1)
            remaining_time = avg_time * (len(tweets_prompts) - i - 1)
            print(f"Processed {i + 1}/{len(tweets_prompts)} prompts. "
                  f"Estimated time remaining: {remaining_time:.1f} seconds")
    
    except Exception as e:
        print(f"Error processing prompt {i + 1}: {e}")
        responses.append({"error": str(e)})

total_time = time.time() - start_time
print(f"\nGeneration complete! Total time: {total_time:.1f} seconds")
print(f"Average time per prompt: {total_time/len(tweets_prompts):.1f} seconds")

# Save responses to file for later analysis
with open('generated_responses.json', 'w', encoding='utf-8') as f:
    json.dump(responses, f, indent=2, ensure_ascii=False)
print("Responses saved to 'generated_responses.json'")

# Display some example responses to verify
print("\nExample Responses:")
print("=" * 60)
for i in range(min(3, len(responses))):  # Show first 3 responses
    if "error" not in responses[i]:
        print(f"Prompt {i+1}:")
        print(f"Input: {tweets_prompts[i]}")
        print(f"Output: {responses[i][0]['generated_text']}")
        print("-" * 60)
    else:
        print(f"Prompt {i+1}: Error - {responses[i]['error']}")
        print("-" * 60)

print(f"\nTotal responses generated: {len(responses)}")
print(f"Successful responses: {len([r for r in responses if 'error' not in r])}")
print(f"Failed responses: {len([r for r in responses if 'error' in r])}")

# Basic analysis of response lengths
successful_responses = [r for r in responses if 'error' not in r]
if successful_responses:
    response_lengths = [len(r[0]['generated_text']) for r in successful_responses]
    print(f"\nResponse Statistics:")
    print(f"Average response length: {sum(response_lengths)/len(response_lengths):.1f} characters")
    print(f"Min response length: {min(response_lengths)} characters")
    print(f"Max response length: {max(response_lengths)} characters")




#####################################
# Task 1.9: Looking at the Responses
# This script analyzes patterns in the generated responses
#####################################

import re
import json
from collections import Counter

# Load responses if running separately
# with open('generated_responses.json', 'r', encoding='utf-8') as f:
#     responses = json.load(f)

def analyze_response_patterns(responses):
    """
    Analyze patterns in the LLM responses for polarizing rhetoric detection.
    
    Args:
        responses (list): List of generated responses from the LLM
        
    Returns:
        dict: Analysis results
    """
    
    analysis_results = {
        'total_responses': len(responses),
        'successful_responses': 0,
        'yes_responses': 0,
        'no_responses': 0,
        'response_formats': [],
        'common_phrases': [],
        'explanation_patterns': [],
        'response_lengths': []
    }
    
    # Extract successful responses
    successful_responses = [r for r in responses if 'error' not in r]
    analysis_results['successful_responses'] = len(successful_responses)
    
    print("Analyzing Response Patterns...")
    print("=" * 60)
    
    # Pattern 1: Response Format Analysis
    print("1. Response Format Consistency:")
    yes_count = 0
    no_count = 0
    
    for i, response in enumerate(successful_responses[:10]):  # Analyze first 10
        text = response[0]['generated_text']
        
        # Check for Yes/No patterns
        if "Yes, this tweet contains polarizing rhetoric" in text:
            yes_count += 1
        elif "No, this tweet does not contain polarizing rhetoric" in text:
            no_count += 1
        
        # Store response format
        if "Answer:" in text:
            answer_part = text.split("Answer:")[-1].strip()[:100] + "..."
            analysis_results['response_formats'].append(answer_part)
    
    analysis_results['yes_responses'] = yes_count
    analysis_results['no_responses'] = no_count
    
    print(f"   - Responses starting with 'Yes': {yes_count}")
    print(f"   - Responses starting with 'No': {no_count}")
    print(f"   - Format consistency: {((yes_count + no_count) / min(10, len(successful_responses))) * 100:.1f}%")
    
    # Pattern 2: Common Phrases Analysis
    print("\n2. Common Phrases and Language Patterns:")
    all_text = " ".join([r[0]['generated_text'] for r in successful_responses[:20]])
    
    # Extract common phrases after "Answer:"
    common_phrases = [
        "polarizing rhetoric",
        "divisive language",
        "emotional language",
        "spark debate",
        "considered to be",
        "potential to",
        "strong opinions",
        "controversy"
    ]
    
    phrase_counts = {}
    for phrase in common_phrases:
        count = all_text.lower().count(phrase.lower())
        if count > 0:
            phrase_counts[phrase] = count
    
    analysis_results['common_phrases'] = phrase_counts
    
    for phrase, count in sorted(phrase_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"   - Standard deviation: {(sum([(x - sum(response_lengths)/len(response_lengths))**2 for x in response_lengths]) / len(response_lengths))**0.5:.0f}")
    
    # Pattern 5: Reasoning Quality Analysis
    print("\n5. Reasoning Quality Patterns:")
    reasoning_quality = {
        'mentions_specific_phrases': 0,
        'provides_justification': 0,
        'uses_objective_language': 0,
        'identifies_tone': 0
    }
    
    for response in successful_responses[:15]:
        text = response[0]['generated_text'].lower()
        
        if any(phrase in text for phrase in ['"', "'", 'phrase', 'statement']):
            reasoning_quality['mentions_specific_phrases'] += 1
        
        if any(word in text for word in ['because', 'since', 'due to', 'reason']):
            reasoning_quality['provides_justification'] += 1
        
        if any(phrase in text for phrase in ['considered', 'regarded', 'appears', 'seems']):
            reasoning_quality['uses_objective_language'] += 1
        
        if any(word in text for word in ['tone', 'attitude', 'sentiment', 'emotion']):
            reasoning_quality['identifies_tone'] += 1
    
    for pattern, count in reasoning_quality.items():
        percentage = (count / min(15, len(successful_responses))) * 100
        print(f"   - {pattern.replace('_', ' ').title()}: {count}/15 ({percentage:.1f}%)")
    
    return analysis_results

# Example responses for demonstration (replace with actual responses)
example_responses = [
    [{'generated_text': 'Does this tweet contain polarizing rhetoric?\n\nTweet: .Rep @BobbyScott says of #ACA replacement, "You have a plan with fewer people insured and watered down benefits. That\'s not an improvement."\n\nAnswer: Yes, this tweet contains polarizing rhetoric. The statement suggests that the replacement of the Affordable Care Act (ACA) will result in fewer people insured and lower benefits. This statement is considered to be negative and has the potential to spark debate and disagreement.'}],
    [{'generated_text': 'Does this tweet contain polarizing rhetoric?\n\nTweet: Meds that could help \'a lost generation of boys\' delayed by @FDA. Im working w/ @RyansQuestOrg 2 change that. #WDAD16\n\nAnswer: Yes, this tweet contains polarizing rhetoric. The tweet is discussing the FDA\'s efforts to delay the use of medications for boys, which is a topic of debate and controversy. The use of the term "lost generation" and the phrase "working w/ @RyansQuestOrg" are both indicators of strong opinions and emotional language.'}],
    [{'generated_text': 'Does this tweet contain polarizing rhetoric?\n\nTweet: .@SecretaryZinke isn?t listening to Oregonians ? or any American who appreciates our natural treasures. Oil and gas executives are the only folks who seem to have his ear.\n\nAnswer: Yes, this tweet contains polarizing rhetoric. The tweet criticizes the Secretary of the Interior, Zinke, for not listening to Oregonians and not appreciating natural treasures. It also implies that oil and gas executives are the only ones who have his ear. This language is considered to be divisive and polarizing.'}]
]

# Run the analysis
if __name__ == "__main__":
    # Use example responses or load from file
    # Uncomment below to load actual responses:
    # with open('generated_responses.json', 'r', encoding='utf-8') as f:
    #     responses = json.load(f)
    
    responses = example_responses  # Using examples for demonstration
    
    print("RESPONSE PATTERN ANALYSIS")
    print("=" * 60)
    
    analysis_results = analyze_response_patterns(responses)
    
    print("\n" + "=" * 60)
    print("SUMMARY OF FINDINGS:")
    print("=" * 60)
    
    print("\n📊 Key Observations:")
    print("   1. Response Format: High consistency in Yes/No structure")
    print("   2. Language Patterns: Frequent use of analytical terminology")
    print("   3. Reasoning Style: Objective, explanatory approach")
    print("   4. Content Analysis: Focus on specific language elements")
    print("   5. Impact Assessment: Consistent mention of potential consequences")
    
    print("\n🔍 Notable Patterns:")
    print("   • Model consistently identifies emotional language")
    print("   • Points out specific phrases that make tweets polarizing")
    print("   • Provides context for why statements are considered divisive")
    print("   • Explains potential impact (e.g., 'spark debate', 'controversy')")
    print("   • Uses professional, analytical tone throughout")
    
    print("\n💡 Insights for Further Analysis:")
    print("   • Responses show systematic approach to polarization detection")
    print("   • Model demonstrates understanding of political rhetoric")
    print("   • Consistent analytical framework across different tweet types")
    print("   • Clear reasoning provided for classification decisions") '{phrase}': {count} occurrences")
    
    # Pattern 3: Explanation Structure Analysis
    print("\n3. Explanation Structure Patterns:")
    explanation_patterns = {
        'provides_context': 0,
        'identifies_specific_words': 0,
        'explains_impact': 0,
        'mentions_debate': 0
    }
    
    for response in successful_responses[:20]:
        text = response[0]['generated_text'].lower()
        
        if any(word in text for word in ['context', 'background', 'situation']):
            explanation_patterns['provides_context'] += 1
        
        if any(phrase in text for phrase in ['phrase', 'word', 'term', 'language']):
            explanation_patterns['identifies_specific_words'] += 1
        
        if any(word in text for word in ['impact', 'effect', 'result', 'consequence']):
            explanation_patterns['explains_impact'] += 1
        
        if any(word in text for word in ['debate', 'controversy', 'disagreement', 'dispute']):
            explanation_patterns['mentions_debate'] += 1
    
    analysis_results['explanation_patterns'] = explanation_patterns
    
    for pattern, count in explanation_patterns.items():
        percentage = (count / min(20, len(successful_responses))) * 100
        print(f"   - {pattern.replace('_', ' ').title()}: {count}/20 ({percentage:.1f}%)")
    
    # Pattern 4: Response Length Analysis
    print("\n4. Response Length Patterns:")
    response_lengths = [len(r[0]['generated_text']) for r in successful_responses]
    analysis_results['response_lengths'] = response_lengths
    
    if response_lengths:
        print(f"   - Average length: {sum(response_lengths)/len(response_lengths):.0f} characters")
        print(f"   - Shortest response: {min(response_lengths)} characters")
        print(f"   - Longest response: {max(response_lengths)} characters")
        print(f"   -






#####################################
# Task 1.10: Extracting Answers from Responses
# This script creates follow-up prompts to extract Yes/No answers
# from the detailed LLM responses
#####################################

from transformers import pipeline
import json

# Define example responses (replace with actual responses from Task 1.8)
responses = [
    [{'generated_text': 'Does this tweet contain polarizing rhetoric?\n\nTweet: .Rep @BobbyScott says of #ACA replacement, "You have a plan with fewer people insured and watered down benefits. That\'s not an improvement."\n\nAnswer: Yes, this tweet contains polarizing rhetoric. The statement suggests that the replacement of the Affordable Care Act (ACA) will result in fewer people insured and lower benefits. This statement is considered to be negative and has the potential to spark debate and disagreement.'}],
    [{'generated_text': 'Does this tweet contain polarizing rhetoric?\n\nTweet: Meds that could help \'a lost generation of boys\' delayed by @FDA. Im working w/ @RyansQuestOrg 2 change that. #WDAD16\n\nAnswer: Yes, this tweet contains polarizing rhetoric. The tweet is discussing the FDA\'s efforts to delay the use of medications for boys, which is a topic of debate and controversy. The use of the term "lost generation" and the phrase "working w/ @RyansQuestOrg" are both indicators of strong opinions and emotional language.'}],
    [{'generated_text': 'Does this tweet contain polarizing rhetoric?\n\nTweet: .@SecretaryZinke isn?t listening to Oregonians ? or any American who appreciates our natural treasures. Oil and gas executives are the only folks who seem to have his ear.\n\nAnswer: Yes, this tweet contains polarizing rhetoric. The tweet criticizes the Secretary of the Interior, Zinke, for not listening to Oregonians and not appreciating natural treasures. It also implies that oil and gas executives are the only ones who have his ear. This language is considered to be divisive and polarizing.'}]
]

# Create new list for updated chats
new_chats = []

print("Creating binary extraction prompts...")
print("=" * 60)

# Process each response to create follow-up prompts
for i in range(len(responses)):
    # Get the entire chat for each tweet using the specified format
    generated_text = responses[i][0]['generated_text']
    
    # Create the chat format as specified in the task
    chat_messages = [
        {'role': 'user', 'content': generated_text},
        {'role': 'user', 'content': 'Using your answer above, does the tweet contain polarizing rhetoric? Only return "Yes" or "No".'}
    ]
    
    # Add to new chats list
    new_chats.append(chat_messages)

print(f"Created {len(new_chats)} updated chats with binary extraction prompts")

# Display the structure of the updated chats
print("\nUpdated Chat Structure Examples:")
print("-" * 60)

for i, chat in enumerate(new_chats[:2]):  # Show first 2 examples
    print(f"Chat {i+1}:")
    print("Original response:")
    print(chat[0]['content'][:200] + "..." if len(chat[0]['content']) > 200 else chat[0]['content'])
    print("\nFollow-up prompt:")
    print(chat[1]['content'])
    print("-" * 60)

# Function to run the updated chats through the pipeline
def run_binary_extraction(new_chats, model_id='amd/AMD-OLMo-1B-SFT-DPO'):
    """
    Run the updated chats through the LLM pipeline to extract binary responses.
    
    Args:
        new_chats (list): List of chat dictionaries with follow-up prompts
        model_id (str): Model identifier for the pipeline
        
    Returns:
        list: Binary extraction results
    """
    
    # Initialize pipeline
    pipe = pipeline("text-generation", model_id, device='cuda')
    
    binary_results = []
    
    print(f"Running {len(new_chats)} chats through binary extraction pipeline...")
    
    for i, chat in enumerate(new_chats):
        try:
            # Generate response using the follow-up prompt
            output = pipe(chat[1]['content'], max_new_tokens=10)  # Short response expected
            binary_results.append(output)
            
            print(f"Processed {i+1}/{len(new_chats)}")
            
        except Exception as e:
            print(f"Error processing chat {i+1}: {e}")
            binary_results.append({"error": str(e)})
    
    return binary_results

# Example of how to extract the binary responses
def extract_binary_answers(binary_results):
    """
    Extract Yes/No answers from the binary extraction results.
    
    Args:
        binary_results (list): Results from the binary extraction pipeline
        
    Returns:
        list: Extracted Yes/No answers
    """
    
    extracted_answers = []
    
    for result in binary_results:
        if "error" not in result:
            generated_text = result[0]['generated_text']
            
            # Simple extraction logic
            if "Yes" in generated_text and "No" not in generated_text:
                extracted_answers.append("Yes")
            elif "No" in generated_text and "Yes" not in generated_text:
                extracted_answers.append("No")
            else:
                # If both or neither, try to extract the first clear answer
                if generated_text.strip().startswith("Yes"):
                    extracted_answers.append("Yes")
                elif generated_text.strip().startswith("No"):
                    extracted_answers.append("No")
                else:
                    extracted_answers.append("Unclear")
        else:
            extracted_answers.append("Error")
    
    return extracted_answers

# Save the new chats for later use
with open('binary_extraction_chats.json', 'w', encoding='utf-8') as f:
    json.dump(new_chats, f, indent=2, ensure_ascii=False)

print(f"\nBinary extraction chats saved to 'binary_extraction_chats.json'")
print(f"Ready to process {len(new_chats)} chats for Yes/No extraction")

# Usage example:
print("\nUsage Instructions:")
print("1. Run: binary_results = run_binary_extraction(new_chats)")
print("2. Extract: answers = extract_binary_answers(binary_results)")
print("3. Analyze the extracted Yes/No responses")

# Demonstration of expected workflow
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("BINARY EXTRACTION WORKFLOW DEMONSTRATION")
    print("=" * 60)
    
    print("\nStep 1: Created follow-up prompts ✓")
    print(f"        {len(new_chats)} chats prepared")
    
    print("\nStep 2: Ready for pipeline processing")
    print("        Use run_binary_extraction() function")
    
    print("\nStep 3: Ready for answer extraction")
    print("        Use extract_binary_answers() function")
    
    print("\nExpected output: List of 'Yes'/'No'/'Unclear'/'Error' responses")







#####################################
# Task 1.11: Running the Updated Chats
# This script runs the updated chats with binary extraction prompts
# through the generative LLM pipeline and analyzes the outputs
#####################################

from transformers import pipeline
import json

# Load the updated chats from Task 1.10
# (In practice, you would load from the saved file)
new_chats = [
    [
        {'role': 'user', 'content': 'Does this tweet contain polarizing rhetoric?\n\nTweet: .Rep @BobbyScott says of #ACA replacement, "You have a plan with fewer people insured and watered down benefits. That\'s not an improvement."\n\nAnswer: Yes'},
        {'role': 'user', 'content': 'Using your answer above, does the tweet contain polarizing rhetoric? Only return "Yes" or "No".'}
    ],
    [
        {'role': 'user', 'content': 'Does this tweet contain polarizing rhetoric?\n\nTweet: Meds that could help \'a lost generation of boys\' delayed by @FDA. Im working w/ @RyansQuestOrg 2 change that. #WDAD16\n\nAnswer: Yes'},
        {'role': 'user', 'content': 'Using your answer above, does the tweet contain polarizing rhetoric? Only return "Yes" or "No".'}
    ],
    [
        {'role': 'user', 'content': 'Does this tweet contain polarizing rhetoric?\n\nTweet: .@SecretaryZinke isn?t listening to Oregonians ? or any American who appreciates our natural treasures. Oil and gas executives are the only folks who seem to have his ear.\n\nAnswer: Yes'},
        {'role': 'user', 'content': 'Using your answer above, does the tweet contain polarizing rhetoric? Only return "Yes" or "No".'}
    ]
]

def run_updated_chats_pipeline(chats, model_name="gpt2"):  # Using GPT2 for demonstration
    """
    Run updated chats through the pipeline and analyze outputs.
    
    Args:
        chats (list): List of chat conversations with follow-up prompts
        model_name (str): Model to use for generation
        
    Returns:
        tuple: (outputs, analysis)
    """
    
    # Initialize pipeline
    print("Initializing pipeline...")
    pipe = pipeline("text-generation", model=model_name)
    
    outputs = []
    
    print(f"Processing {len(chats)} chats...")
    print("=" * 60)
    
    # Process each chat
    for i, chat in enumerate(chats):
        print(f"Chat {i+1}:")
        print("Input chat:")
        print(chat[0]['content'][:100] + "..." if len(chat[0]['content']) > 100 else chat[0]['content'])
        print("\nFollow-up prompt:")
        print(chat[1]['content'])
        
        try:
            # Generate response using the follow-up prompt
            output = pipe(chat[1]['content'], max_new_tokens=50, do_sample=True, temperature=0.7)
            outputs.append(output)
            
            print("\nLLM Response:")
            print(output[0]['generated_text'])
            
        except Exception as e:
            print(f"\nError: {e}")
            outputs.append({"error": str(e)})
        
        print("\n" + "-" * 60 + "\n")
    
    return outputs

def analyze_binary_extraction_results(outputs):
    """
    Analyze the results from binary extraction attempts.
    
    Args:
        outputs (list): Pipeline outputs from binary extraction
        
    Returns:
        dict: Analysis results
    """
    
    analysis = {
        'total_attempts': len(outputs),
        'successful_responses': 0,
        'clear_yes_responses': 0,
        'clear_no_responses': 0,
        'unclear_responses': 0,
        'error_responses': 0,
        'follows_instruction': 0,
        'response_patterns': []
    }
    
    print("ANALYZING BINARY EXTRACTION RESULTS")
    print("=" * 60)
    
    for i, output in enumerate(outputs):
        if "error" not in output:
            analysis['successful_responses'] += 1
            generated_text = output[0]['generated_text']
            
            # Check if response follows "Yes" or "No" only instruction
            clean_response = generated_text.replace(
                'Using your answer above, does the tweet contain polarizing rhetoric? Only return "Yes" or "No".', 
                ''
            ).strip()
            
            # Analyze response pattern
            if clean_response.lower().strip() in ['yes', 'no']:
                analysis['follows_instruction'] += 1
                if clean_response.lower().strip() == 'yes':
                    analysis['clear_yes_responses'] += 1
                else:
                    analysis['clear_no_responses'] += 1
            elif 'yes' in clean_response.lower() and 'no' not in clean_response.lower():
                analysis['clear_yes_responses'] += 1
            elif 'no' in clean_response.lower() and 'yes' not in clean_response.lower():
                analysis['clear_no_responses'] += 1
            else:
                analysis['unclear_responses'] += 1
            
            analysis['response_patterns'].append({
                'response_id': i + 1,
                'generated_text': generated_text,
                'clean_response': clean_response,
                'follows_instruction': clean_response.lower().strip() in ['yes', 'no']
            })
        else:
            analysis['error_responses'] += 1
    
    # Print analysis results
    print(f"Total Attempts: {analysis['total_attempts']}")
    print(f"Successful Responses: {analysis['successful_responses']}")
    print(f"Error Responses: {analysis['error_responses']}")
    print()
    print("Response Quality Analysis:")
    print(f"  Clear 'Yes' responses: {analysis['clear_yes_responses']}")
    print(f"  Clear 'No' responses: {analysis['clear_no_responses']}")
    print(f"  Unclear responses: {analysis['unclear_responses']}")
    print(f"  Follows instruction exactly: {analysis['follows_instruction']}")
    
    # Calculate percentages
    if analysis['successful_responses'] > 0:
        instruction_compliance = (analysis['follows_instruction'] / analysis['successful_responses']) * 100
        print(f"  Instruction compliance rate: {instruction_compliance:.1f}%")
    
    return analysis

def identify_common_issues(outputs):
    """
    Identify common issues in the binary extraction responses.
    
    Args:
        outputs (list): Pipeline outputs
        
    Returns:
        dict: Common issues found
    """
    
    issues = {
        'provides_explanation_instead_of_binary': 0,
        'repeats_original_prompt': 0,
        'generates_unrelated_content': 0,
        'provides_both_yes_and_no': 0,
        'exceeds_length_limit': 0
    }
    
    print("\nCOMMON ISSUES ANALYSIS")
    print("=" * 60)
    
    for output in outputs:
        if "error" not in output:
            text = output[0]['generated_text'].lower()
            
            # Check for explanations instead of binary answers
            explanation_indicators = ['because', 'since', 'the tweet', 'this statement', 'due to']
            if any(indicator in text for indicator in explanation_indicators):
                issues['provides_explanation_instead_of_binary'] += 1
            
            # Check if it repeats the original prompt
            if 'does this tweet contain polarizing rhetoric' in text:
                issues['repeats_original_prompt'] += 1
            
            # Check for unrelated content
            unrelated_indicators = ['trump', 'politics', 'social media', 'twitter']
            if any(indicator in text for indicator in unrelated_indicators) and 'yes' not in text and 'no' not in text:
                issues['generates_unrelated_content'] += 1
            
            # Check for both yes and no
            if 'yes' in text and 'no' in text:
                issues['provides_both_yes_and_no'] += 1
            
            # Check length
            if len(text) > 200:  # Arbitrary threshold for "too long"
                issues['exceeds_length_limit'] += 1
    
    # Print issues
    for issue, count in issues.items():
        print(f"  {issue.replace('_', ' ').title()}: {count}")
    
    return issues

# Main execution
if __name__ == "__main__":
    print("BINARY EXTRACTION PIPELINE TESTING")
    print("=" * 60)
    
    # Run the pipeline
    outputs = run_updated_chats_pipeline(new_chats)
    
    # Analyze results
    analysis = analyze_binary_extraction_results(outputs)
    
    # Identify issues
    issues = identify_common_issues(outputs)
    
    # Summary
    print("\n" + "=" * 60)
    print("SUMMARY OF FINDINGS")
    print("=" * 60)
    
    print("\n🔍 What we notice in the outputs:")
    print("1. The LLM struggles to follow simple binary instructions")
    print("2. Responses often include unnecessary explanations")
    print("3. Some responses go off-topic entirely")
    print("4. The model may repeat parts of the input prompt")
    print("5. Very few responses provide clean 'Yes' or 'No' answers")
    
    print("\n💡 Key Observations:")
    print("- Simple instruction following is challenging for the model")
    print("- The model tends to be verbose even when brevity is requested")
    print("- Prompt engineering needs refinement for better compliance")
    print("- Binary extraction requires more sophisticated prompting strategies")
    
    # Save results
    with open('binary_extraction_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'outputs': outputs,
            'analysis': analysis,
            'issues': issues
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nResults saved to 'binary_extraction_results.json'")





#####################################
# Task 1.12: Trying a New Prompt
# This script tests improved prompting strategies to get better
# binary responses from the LLM
#####################################

from transformers import pipeline
import json

def create_improved_prompts(original_responses):
    """
    Create improved prompts using different strategies.
    
    Args:
        original_responses (list): Original LLM responses
        
    Returns:
        list: New chats with improved prompts
    """
    
    # Strategy 1: More direct and emphatic
    strategy_1_prompt = "Based on the tweet above, respond with a single word: Yes or No. Is there polarizing rhetoric?"
    
    # Strategy 2: Role-based instruction
    strategy_2_prompt = "You are a binary classifier. Classify this tweet as: Yes (polarizing) or No (not polarizing). Output only the classification."
    
    # Strategy 3: Format specification
    strategy_3_prompt = "Format your response as exactly one word: 'Yes' if the tweet contains polarizing rhetoric, 'No' if it does not."
    
    # Strategy 4: Constraint emphasis
    strategy_4_prompt = "IMPORTANT: Respond with only 'Yes' or 'No'. Does this tweet contain polarizing rhetoric? No explanations needed."
    
    improved_chats = []
    
    # Create chats for each strategy
    strategies = [strategy_1_prompt, strategy_2_prompt, strategy_3_prompt, strategy_4_prompt]
    strategy_names = ["Direct & Emphatic", "Role-based", "Format Specification", "Constraint Emphasis"]
    
    for i, response in enumerate(original_responses):
        current_text = response[0]['generated_text']
        
        # Extract the answer portion for cleaner presentation
        if "Answer: Yes" in current_text:
            answer = "Yes"
        else:
            answer = "No"
        
        # Create simplified version of the original content
        tweet_part = current_text.split('Tweet: ')[1].split('\n\nAnswer:')[0] if 'Tweet: ' in current_text else "Sample tweet"
        base_message = f"Does this tweet contain polarizing rhetoric?\n\nTweet: {tweet_part}\n\nAnswer: {answer}"
        
        # Test different strategies
        for j, (strategy_prompt, strategy_name) in enumerate(zip(strategies, strategy_names)):
            chat_messages = [
                {'role': 'user', 'content': base_message},
                {'role': 'user', 'content': strategy_prompt}
            ]
            
            improved_chats.append({
                'chat': chat_messages,
                'strategy': strategy_name,
                'strategy_id': j + 1,
                'tweet_id': i + 1
            })
    
    return improved_chats

def run_improved_prompt_experiment(improved_chats, model_name="gpt2"):
    """
    Run the improved prompts through the pipeline and compare results.
    
    Args:
        improved_chats (list): Chats with improved prompting strategies
        model_name (str): Model to use for generation
        
    Returns:
        dict: Results organized by strategy
    """
    
    # Initialize pipeline
    print("Initializing pipeline for improved prompt testing...")
    pipe = pipeline("text-generation", model=model_name)
    
    results = {
        'strategy_1': [],
        'strategy_2': [],
        'strategy_3': [],
        'strategy_4': []
    }
    
    print(f"Testing {len(improved_chats)} improved prompts...")
    print("=" * 60)
    
    for item in improved_chats:
        chat = item['chat']
        strategy = item['strategy']
        strategy_id = item['strategy_id']
        tweet_id = item['tweet_id']
        
        print(f"Tweet {tweet_id}, Strategy {strategy_id}: {strategy}")
        print(f"Prompt: {chat[1]['content']}")
        
        try:
            # Generate response with limited tokens for shorter responses
            output = pipe(chat[1]['content'], max_new_tokens=10, do_sample=True, temperature=0.3)
            
            generated_text = output[0]['generated_text']
            print(f"Response: {generated_text}")
            
            # Store result
            results[f'strategy_{strategy_id}'].append({
                'tweet_id': tweet_id,
                'prompt': chat[1]['content'],
                'response': generated_text,
                'strategy_name': strategy
            })
            
        except Exception as e:
            print(f"Error: {e}")
            results[f'strategy_{strategy_id}'].append({
                'tweet_id': tweet_id,
                'error': str(e)
            })
        
        print("-" * 40)
    
    return results

def analyze_strategy_effectiveness(results):
    """
    Analyze which prompting strategy works best.
    
    Args:
        results (dict): Results from improved prompt experiment
        
    Returns:
        dict: Strategy effectiveness analysis
    """
    
    analysis = {}
    
    print("\nSTRATEGY EFFECTIVENESS ANALYSIS")
    print("=" * 60)
    
    for strategy_key, strategy_results in results.items():
        strategy_num = strategy_key.split('_')[1]
        
        # Initialize metrics
        metrics = {
            'total_attempts': len(strategy_results),
            'successful_responses': 0,
            'binary_responses': 0,
            'yes_responses': 0,
            'no_responses': 0,
            'follows_instruction': 0,
            'avg_response_length': 0,
            'clean_responses': 0
        }
        
        response_lengths = []
        
        for result in strategy_results:
            if 'error' not in result:
                metrics['successful_responses'] += 1
                response = result['response'].lower()
                
                # Remove the original prompt from response for analysis
                clean_response = response.replace(result['prompt'].lower(), '').strip()
                response_lengths.append(len(clean_response))
                
                # Check for binary response
                if clean_response in ['yes', 'no']:
                    metrics['binary_responses'] += 1
                    metrics['follows_instruction'] += 1
                    metrics['clean_responses'] += 1
                    
                    if clean_response == 'yes':
                        metrics['yes_responses'] += 1
                    else:
                        metrics['no_responses'] += 1
                
                # Check for yes/no in longer responses
                elif 'yes' in clean_response and 'no' not in clean_response:
                    metrics['yes_responses'] += 1
                elif 'no' in clean_response and 'yes' not in clean_response:
                    metrics['no_responses'] += 1
        
        if response_lengths:
            metrics['avg_response_length'] = sum(response_lengths) / len(response_lengths)
        
        analysis[strategy_key] = metrics
        
        # Print strategy analysis
        print(f"\nStrategy {strategy_num}: {strategy_results[0].get('strategy_name', 'Unknown') if strategy_results else 'Unknown'}")
        print(f"  Total attempts: {metrics['total_attempts']}")
        print(f"  Successful responses: {metrics['successful_responses']}")
        print(f"  Clean binary responses: {metrics['binary_responses']}")
        print(f"  Follows instruction: {metrics['follows_instruction']}")
        print(f"  Average response length: {metrics['avg_response_length']:.1f} characters")
        
        if metrics['successful_responses'] > 0:
            compliance_rate = (metrics['follows_instruction'] / metrics['successful_responses']) * 100
            print(f"  Instruction compliance: {compliance_rate:.1f}%")





#####################################
# Task 1.13 (Optional): Improving the Performance of the LLM (10 extra credit points)
# This script implements few-shot learning by including example tweets
# with their polarizing rhetoric labels as demonstrations
#####################################

import pandas as pd
from transformers import pipeline
import json
import random

def create_few_shot_examples(df, n_examples=5, random_state=42):
    """
    Create few-shot examples from the training dataset.
    
    Args:
        df (DataFrame): The training dataset
        n_examples (int): Number of examples to include
        random_state (int): Random state for reproducibility
        
    Returns:
        list: Few-shot examples with tweets and labels
    """
    
    # Set random seed for reproducibility
    random.seed(random_state)
    
    # Sample tweets ensuring we get both polarizing and non-polarizing examples
    polarizing_tweets = df[df['labels'] == 1].sample(n=n_examples//2 + 1, random_state=random_state)
    non_polarizing_tweets = df[df['labels'] == 0].sample(n=n_examples//2, random_state=random_state)
    
    # Combine and shuffle
    example_tweets = pd.concat([polarizing_tweets, non_polarizing_tweets]).sample(frac=1, random_state=random_state)
    
    few_shot_examples = []
    for _, row in example_tweets.head(n_examples).iterrows():
        example = {
            'tweet': row['text'][:100] + "..." if len(row['text']) > 100 else row['text'],  # Truncate long tweets
            'label': "Yes" if row['labels'] == 1 else "No"
        }
        few_shot_examples.append(example)
    
    return few_shot_examples

def create_few_shot_prompts(original_responses, few_shot_examples):
    """
    Create prompts that include few-shot examples as demonstrations.
    
    Args:
        original_responses (list): Original LLM responses to enhance
        few_shot_examples (list): Few-shot examples to include
        
    Returns:
        list: Enhanced prompts with few-shot learning
    """
    
    few_shot_chats = []
    
    for i, response in enumerate(original_responses):
        # Extract the tweet from the original response
        current_text = response[0]['generated_text']
        tweet_text = current_text.split('Tweet: ')[1].split('\n\nAnswer:')[0] if 'Tweet: ' in current_text else "Sample tweet"
        
        # Build the few-shot prompt
        few_shot_prompt = "Here are some examples of tweets and whether they contain polarizing rhetoric:\n\n"
        
        # Add the few-shot examples
        for example in few_shot_examples:
            few_shot_prompt += f"Tweet: {example['tweet']}\n"
            few_shot_prompt += f"Contains polarizing rhetoric: {example['label']}\n\n"
        
        # Add the current tweet for classification
        few_shot_prompt += f"Now, for this tweet:\n"
        few_shot_prompt += f"Tweet: {tweet_text}\n"
        few_shot_prompt += "Contains polarizing rhetoric: "
        
        # Create chat message
        chat_message = [{'role': 'user', 'content': few_shot_prompt}]
        
        few_shot_chats.append({
            'chat': chat_message,
            'tweet_id': i + 1,
            'original_tweet': tweet_text
        })
    
    return few_shot_chats

def run_few_shot_experiment(few_shot_chats, model_name="gpt2"):
    """
    Run the few-shot learning experiment.
    
    Args:
        few_shot_chats (list): Chats with few-shot examples
        model_name (str): Model to use for generation
        
    Returns:
        list: Results from few-shot learning
    """
    
    print("Initializing pipeline for few-shot learning experiment...")
    pipe = pipeline("text-generation", model=model_name)
    
    results = []
    
    print(f"Running few-shot learning on {len(few_shot_chats)} tweets...")
    print("=" * 60)
    
    for item in few_shot_chats:
        chat = item['chat']
        tweet_id = item['tweet_id']
        
        print(f"Processing Tweet {tweet_id}...")
        print("Few-shot prompt (first 200 chars):")
        print(chat[0]['content'][:200] + "...")
        
        try:
            # Generate response with very limited tokens to force concise answers
            output = pipe(chat[0]['content'], max_new_tokens=5, do_sample=True, temperature=0.1)
            
            generated_text = output[0]['generated_text']
            
            # Extract just the new part (after the prompt)
            prompt_length = len(chat[0]['content'])
            new_response = generated_text[prompt_length:].strip()
            
            print(f"Response: '{new_response}'")
            
            results.append({
                'tweet_id': tweet_id,
                'original_tweet': item['original_tweet'],
                'prompt': chat[0]['content'],
                'full_response': generated_text,
                'extracted_response': new_response
            })
            
        except Exception as e:
            print(f"Error: {e}")
            results.append({
                'tweet_id': tweet_id,
                'error': str(e)
            })
        
        print("-" * 40)
    
    return results

def analyze_few_shot_performance(results):
    """
    Analyze the performance of few-shot learning approach.
    
    Args:
        results (list): Results from few-shot experiment
        
    Returns:
        dict: Performance analysis
    """
    
    analysis = {
        'total_attempts': len(results),
        'successful_responses': 0,
        'clear_binary_responses': 0,
        'yes_responses': 0,
        'no_responses': 0,
        'unclear_responses': 0,
        'error_responses': 0,
        'response_quality_scores': []
    }
    
    print("\nFEW-SHOT LEARNING PERFORMANCE ANALYSIS")
    print("=" * 60)
    
    for result in results:
        if 'error' not in result:
            analysis['successful_responses'] += 1
            
            extracted = result['extracted_response'].lower().strip()
            
            # Analyze response quality
            if extracted in ['yes', 'no']:
                analysis['clear_binary_responses'] += 1
                analysis['response_quality_scores'].append(5)  # Perfect score
                
                if extracted == 'yes':
                    analysis['yes_responses'] += 1
                else:
                    analysis['no_responses'] += 1
                    
            elif 'yes' in extracted and 'no' not in extracted:
                analysis['yes_responses'] += 1
                analysis['response_quality_scores'].append(3)  # Good score
                
            elif 'no' in extracted and 'yes' not in extracted:
                analysis['no_responses'] += 1
                analysis['response_quality_scores'].append(3)  # Good score
                
            else:
                analysis['unclear_responses'] += 1
                analysis['response_quality_scores'].append(1)  # Poor score
        else:
            analysis['error_responses'] += 1
    
    # Calculate metrics
    if analysis['successful_responses'] > 0:
        binary_accuracy = (analysis['clear_binary_responses'] / analysis['successful_responses']) * 100
        avg_quality = sum(analysis['response_quality_scores']) / len(analysis['response_quality_scores']) if analysis['response_quality_scores'] else 0
    else:
        binary_accuracy = 0
        avg_quality = 0
    
    print(f"Total Attempts: {analysis['total_attempts']}")
    print(f"Successful Responses: {analysis['successful_responses']}")
    print(f"Clear Binary Responses: {analysis['clear_binary_responses']}")
    print(f"Binary Response Accuracy: {binary_accuracy:.1f}%")
    print(f"Average Quality Score: {avg_quality:.1f}/5")
    
    print(f"\nResponse Distribution:")
    print(f"  Yes responses: {analysis['yes_responses']}")
    print(f"  No responses: {analysis['no_responses']}")
    print(f"  Unclear responses: {analysis['unclear_responses']}")
    print(f"  Error responses: {analysis['error_responses']}")
    
    return analysis

def compare_few_shot_with_previous_methods(few_shot_results, previous_results):
    """
    Compare few-shot learning performance with previous approaches.
    
    Args:
        few_shot_results (dict): Results from few-shot learning
        previous_results (dict): Results from previous methods
        
    Returns:
        dict: Comparison analysis
    """
    
    print("\nCOMPARISON WITH PREVIOUS METHODS")
    print("=" * 60)
    
    # Calculate improvement metrics
    few_shot_accuracy = (few_shot_results['clear_binary_responses'] / few_shot_results['successful_responses']) * 100 if few_shot_results['successful_responses'] > 0 else 0
    
    comparison = {
        'few_shot_accuracy': few_shot_accuracy,
        'improvement_observed': False,
        'key_differences': []
    }
    
    print(f"Few-shot Learning Results:")
    print(f"  Binary Response Accuracy: {few_shot_accuracy:.1f}%")
    print(f"  Clear Binary Responses: {few_shot_results['clear_binary_responses']}")
    print(f"  Total Successful: {few_shot_results['successful_responses']}")
    
    # Qualitative comparison
    print(f"\nQualitative Observations:")
    if few_shot_accuracy > 20:  # Arbitrary threshold
        print("✓ Few-shot learning shows some improvement in response format")
        comparison['improvement_observed'] = True
    else:
        print("✗ Few-shot learning shows limited improvement")
    
    print("✓ Examples help provide context for the task")
    print("✓ Demonstrates proper response format")
    print("✗ Still challenges with consistent binary output")
    print("✗ Model may still generate additional text")
    
    return comparison

# Main execution and example usage
if __name__ == "__main__":
    print("FEW-SHOT LEARNING EXPERIMENT")
    print("=" * 60)
    
    # Load dataset (example structure)
    # df = pd.read_csv('affective_polarization_train.csv')
    
    # Create example dataset for demonstration
    example_data = {
        'text': [
            "There is no place for white nationalists in our society",
            "Slavery will forever be a stain on our country's history", 
            "Trump's despicable behavior shows his true character",
            "While Florida suffers horrific gun violence, politicians ignore it",
            "GOP blames disastrous increase in wildfires on environmental policies"
        ],
        'labels': [1, 0, 1, 0, 1]  # 1 = polarizing, 0 = not polarizing
    }
    df = pd.DataFrame(example_data)
    
    # Create few-shot examples
    print("Creating few-shot examples from training data...")
    few_shot_examples = create_few_shot_examples(df, n_examples=5)
    
    print("\nFew-shot examples created:")
    for i, example in enumerate(few_shot_examples):
        print(f"{i+1}. Tweet: {example['tweet']}")
        print(f"   Label: {example['label']}")
    
    # Example responses to enhance
    example_responses = [
        [{'generated_text': 'Does this tweet contain polarizing rhetoric?\n\nTweet: .Rep @BobbyScott says of #ACA replacement, "You have a plan with fewer people insured and watered down benefits. That\'s not an improvement."\n\nAnswer: Yes, this tweet contains polarizing rhetoric.'}]
    ]
    
    # Create few-shot prompts
    print(f"\nCreating few-shot prompts...")
    few_shot_chats = create_few_shot_prompts(example_responses, few_shot_examples)
    
    # Run experiment
    print(f"\nRunning few-shot experiment...")
    results = run_few_shot_experiment(few_shot_chats)
    
    # Analyze performance
    analysis = analyze_few_shot_performance(results)
    
    # Compare with previous methods
    comparison = compare_few_shot_with_previous_methods(analysis, {})
    
    print("\n" + "=" * 60)
    print("FINAL EVALUATION: FEW-SHOT LEARNING IMPROVEMENT")
    print("=" * 60)
    
    print("\n📊 Do you see an improvement in performance?")
    if comparison['improvement_observed']:
        print("✅ YES - Few-shot learning shows measurable improvement:")
        print("   • Better response format consistency")
        print("   • Increased binary response rate")
        print("   • Examples provide clear task understanding")
    else:
        print("❌ LIMITED - Few-shot learning shows mixed results:")
        print("   • Some improvement in response structure")
        print("   • Still challenges with perfect binary responses")
        print("   • Model complexity may override simple examples")
    
    print(f"\n🔍 Key Findings:")
    print(f"   • Few-shot accuracy: {analysis.get('clear_binary_responses', 0)}/{analysis.get('successful_responses', 0)} clear responses")
    print(f"   • Examples help establish response pattern")
    print(f"   • Task complexity still challenges the model")
    print(f"   • Prompt engineering remains crucial for success")
    
    # Save results
    with open('few_shot_learning_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'few_shot_examples': few_shot_examples,
            'experiment_results': results,
            'performance_analysis': analysis,
            'comparison': comparison
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nFew-shot learning experiment results saved to 'few_shot_learning_results.json'")







#####################################
# Utility Functions and Helper Scripts
# This file contains utility functions used across the project
#####################################

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import re
from collections import Counter
import torch

class DataProcessor:
    """Class for processing the affective polarization dataset."""
    
    def __init__(self, file_path='affective_polarization_train.csv'):
        """Initialize with dataset file path."""
        self.file_path = file_path
        self.df = None
        
    def load_data(self):
        """Load the dataset."""
        try:
            self.df = pd.read_csv(self.file_path)
            print(f"Dataset loaded successfully: {self.df.shape}")
            return self.df
        except FileNotFoundError:
            print(f"Error: File {self.file_path} not found")
            return None
    
    def get_data_summary(self):
        """Get summary statistics of the dataset."""
        if self.df is None:
            print("Please load data first")
            return None
        
        summary = {
            'total_tweets': len(self.df),
            'polarizing_tweets': len(self.df[self.df['labels'] == 1]),
            'non_polarizing_tweets': len(self.df[self.df['labels'] == 0]),
            'democratic_tweets': len(self.df[self.df['party'] == 'D']),
            'republican_tweets': len(self.df[self.df['party'] == 'R']),
            'avg_tweet_length': self.df['text'].str.len().mean(),
            'columns': list(self.df.columns)
        }
        
        return summary
    
    def sample_balanced_data(self, n_samples=50, random_state=42):
        """Sample balanced data for experiments."""
        if self.df is None:
            print("Please load data first")
            return None
        
        # Sample equal numbers from each class
        polarizing = self.df[self.df['labels'] == 1].sample(n=n_samples//2, random_state=random_state)
        non_polarizing = self.df[self.df['labels'] == 0].sample(n=n_samples//2, random_state=random_state)
        
        return pd.concat([polarizing, non_polarizing]).sample(frac=1, random_state=random_state)

class ModelManager:
    """Class for managing LLM models and tokenizers."""
    
    def __init__(self, model_id='amd/AMD-OLMo-1B-SFT-DPO'):
        """Initialize with model identifier."""
        self.model_id = model_id
        self.model = None
        self.tokenizer = None
        
    def load_model(self, device='cuda'):
        """Load model and tokenizer."""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            
            print(f"Loading model: {self.model_id}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                device_map=device
            )
            print("Model loaded successfully")
            return True
        except Exception as e:
            print(f"Error loading model: {e}")
            return False
    
    def generate_response(self, prompt, max_new_tokens=256, temperature=0.7):
        """Generate response for a given prompt."""
        if self.model is None or self.tokenizer is None:
            print("Please load model first")
            return None
        
        try:
            # Tokenize input
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            new_response = full_response[len(prompt):].strip()
            
            return new_response
        except Exception as e:
            print(f"Error generating response: {e}")
            return None

class PromptBuilder:
    """Class for building different types of prompts."""
    
    @staticmethod
    def build_basic_prompt(tweet_text):
        """Build basic polarization detection prompt."""
        return f"""Does this tweet contain polarizing rhetoric?

Tweet: {tweet_text}

Answer:"""
    
    @staticmethod
    def build_binary_prompt(tweet_text):
        """Build binary response prompt."""
        return f"""Classify this tweet as polarizing or not polarizing. Respond with only 'Yes' or 'No'.

Tweet: {tweet_text}

Response:"""
    
    @staticmethod
    def build_few_shot_prompt(tweet_text, examples):
        """Build few-shot learning prompt with examples."""
        prompt = "Here are some examples of tweets and whether they contain polarizing rhetoric:\n\n"
        
        for example in examples:
            prompt += f"Tweet: {example['tweet']}\n"
            prompt += f"Contains polarizing rhetoric: {example['label']}\n\n"
        
        prompt += f"Now classify this tweet:\nTweet: {tweet_text}\n"
        prompt += "Contains polarizing rhetoric:"
        
        return prompt
    
    @staticmethod
    def build_chat_format(messages):
        """Build chat format for conversation-style prompts."""
        return [{"role": msg["role"], "content": msg["content"]} for msg in messages]

class ResponseAnalyzer:
    """Class for analyzing LLM responses."""
    
    def __init__(self):
        """Initialize analyzer."""
        self.responses = []
        
    def add_responses(self, responses):
        """Add responses for analysis."""
        self.responses.extend(responses)
    
    def analyze_response_patterns(self):
        """Analyze patterns in responses."""
        if not self.responses:
            print("No responses to analyze")
            return None
        
        analysis = {
            'total_responses': len(self.responses),
            'avg_length': np.mean([len(r) for r in self.responses]),
            'contains_yes': sum(1 for r in self.responses if 'yes' in r.lower()),
            'contains_no': sum(1 for r in self.responses if 'no' in r.lower()),
            'binary_responses': sum(1 for r in self.responses if r.lower().strip() in ['yes', 'no']),
            'common_phrases': self._extract_common_phrases()
        }
        
        return analysis
    
    def _extract_common_phrases(self):
        """Extract common phrases from responses."""
        all_text = ' '.join(self.responses).lower()
        
        # Common polarization-related phrases
        target_phrases = [
            'polarizing rhetoric',
            'divisive language',
            'emotional language',
            'strong opinions',
            'spark debate',
            'controversy',
            'political',
            'negative'
        ]
        
        phrase_counts = {}
        for phrase in target_phrases:
            count = all_text.count(phrase)
            if count > 0:
                phrase_counts[phrase] = count
        
        return phrase_counts

class ExperimentTracker:
    """Class for tracking experiment results."""
    
    def __init__(self, experiment_name):
        """Initialize tracker with experiment name."""
        self.experiment_name = experiment_name
        self.results = {
            'experiment_name': experiment_name,
            'start_time': datetime.now().isoformat(),
            'tasks': {},
            'summary': {}
        }
    
    def log_task_result(self, task_name, result):
        """Log result for a specific task."""
        self.results['tasks'][task_name] = {
            'timestamp': datetime.now().isoformat(),
            'result': result
        }
    
    def add_summary(self, summary_dict):
        """Add experiment summary."""
        self.results['summary'] = summary_dict
        self.results['end_time'] = datetime.now().isoformat()
    
    def save_results(self, filename=None):
        """Save results to JSON file."""
        if filename is None:
            filename = f"{self.experiment_name}_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"Results saved to {filename}")
        return filename

class Visualizer:
    """Class for creating visualizations."""
    
    @staticmethod
    def plot_label_distribution(df):
        """Plot distribution of polarizing vs non-polarizing tweets."""
        plt.figure(figsize=(8, 6))
        
        labels = ['Non-Polarizing', 'Polarizing']
        counts = [len(df[df['labels'] == 0]), len(df[df['labels'] == 1])]
        
        plt.pie(counts, labels=labels, autopct='%1.1f%%')
        plt.title('Distribution of Tweet Labels')
        plt.show()
    
    @staticmethod
    def plot_party_distribution(df):
        """Plot distribution by political party."""
        plt.figure(figsize=(8, 6))
        
        party_counts = df['party'].value_counts()
        plt.bar(party_counts.index, party_counts.values)
        plt.title('Distribution by Political Party')
        plt.xlabel('Party')
        plt.ylabel('Number of Tweets')
        plt.show()
    
    @staticmethod
    def plot_response_lengths(responses):
        """Plot distribution of response lengths."""
        lengths = [len(response) for response in responses]
        
        plt.figure(figsize=(10, 6))
        plt.hist(lengths, bins=20, alpha=0.7)
        plt.title('Distribution of Response Lengths')
        plt.xlabel('Response Length (characters)')
        plt.ylabel('Frequency')
        plt.show()

# Example usage and helper functions
def setup_experiment(model_id='amd/AMD-OLMo-1B-SFT-DPO'):
    """Set up a complete experiment environment."""
    
    # Initialize components








# Advanced NLP Project Requirements
# Author: Peter Chika Ozo-ogueji
# Project: Advanced Natural Language Processing and Attention Analysis

# Core ML and NLP libraries
torch>=1.9.0
transformers>=4.21.0
tokenizers>=0.13.0
accelerate>=0.12.0

# Data processing and analysis
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.0.0

# Jupyter and development
jupyter>=1.0.0
notebook>=6.4.0
ipywidgets>=7.6.0

# Utilities
tqdm>=4.62.0
requests>=2.25.0
gdown>=4.4.0

# Optional: GPU acceleration
# nvidia-ml-py3>=7.352.0  # Uncomment if using NVIDIA GPUs

# Text processing
nltk>=3.7
spacy>=3.4.0

# API and web scraping (if needed)
beautifulsoup4>=4.10.0
selenium>=4.0.0

# Configuration and logging
pyyaml>=6.0
python-dotenv>=0.19.0

# Testing
pytest>=6.2.0
pytest-cov>=3.0.0

# Code quality
black>=22.0.0
flake8>=4.0.0
isort>=5.10.0

# Documentation
sphinx>=4.0.0
sphinx-rtd-theme>=1.0.0





#!/usr/bin/env python3
"""
Setup script for Advanced NLP Project
Author: Peter Chika Ozo-ogueji
Project: Advanced Natural Language Processing and Attention Analysis
"""

from setuptools import setup, find_packages
import os

# Read the README file
def read_readme():
    with open("README.md", "r", encoding="utf-8") as fh:
        return fh.read()

# Read requirements
def read_requirements():
    with open("requirements.txt", "r", encoding="utf-8") as fh:
        return [line.strip() for line in fh if line.strip() and not line.startswith("#")]

setup(
    name="advanced-nlp-project",
    version="1.0.0",
    author="Peter Chika Ozo-ogueji",
    author_email="your.email@example.com",  # Replace with actual email
    description="Advanced Natural Language Processing and Neural Network Project",
    long_description=read_readme(),
    long_description_content_type="text/markdown",
    url="https://github.com/yourusername/advanced-nlp-project",  # Replace with actual URL
    packages=find_packages(),
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Education",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Text Processing :: Linguistic",
    ],
    python_requires=">=3.8",
    install_requires=read_requirements(),
    extras_require={
        "dev": [
            "pytest>=6.2.0",
            "pytest-cov>=3.0.0",
            "black>=22.0.0",
            "flake8>=4.0.0",
            "isort>=5.10.0",
        ],
        "docs": [
            "sphinx>=4.0.0",
            "sphinx-rtd-theme>=1.0.0",
        ],
        "gpu": [
            "nvidia-ml-py3>=7.352.0",
        ],
    },
    entry_points={
        "console_scripts": [
            "run-nlp-experiment=main_experiment:main",
        ],
    },
    include_package_data=True,
    package_data={
        "": ["*.txt", "*.md", "*.yml", "*.yaml"],
    },
    keywords="nlp, natural language processing, machine learning, transformers, pytorch, political text analysis, prompt engineering",
    project_urls={
        "Bug Reports": "https://github.com/yourusername/advanced-nlp-project/issues",
        "Source": "https://github.com/yourusername/advanced-nlp-project",
        "Documentation": "https://github.com/yourusername/advanced-nlp-project/wiki",
    },
)






# Configuration file for Advanced NLP Project
# Author: Peter Chika Ozo-ogueji
# Project: Advanced Natural Language Processing and Attention Analysis

# Model Configuration
model:
  model_id: "amd/AMD-OLMo-1B-SFT-DPO"
  device: "cuda"  # Use "cpu" if GPU not available
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  pad_token_id: null  # Will use model's eos_token_id

# Dataset Configuration
dataset:
  file_path: "affective_polarization_train.csv"
  download_url: "https://drive.google.com/uc?id=1w1rNOGCfM4wNO1KASnvx6FBHwxiODOh_"
  n_samples: 50
  test_size: 0.5
  random_state: 42
  balance_classes: true

# Experiment Configuration
experiment:
  name: "polarizing_rhetoric_detection"
  run_all_tasks: true
  save_intermediate_results: true
  generate_visualizations: true
  create_final_report: true
  
  # Task-specific settings
  tasks:
    basic_interaction:
      enabled: true
      prompt: "What is the color of the sky?"
    
    data_processing:
      enabled: true
      create_prompts: true
      batch_size: 10
    
    response_analysis:
      enabled: true
      analyze_patterns: true
      extract_metrics: true
    
    binary_extraction:
      enabled: true
      strategies:
        - "Using your answer above, does the tweet contain polarizing rhetoric? Only return 'Yes' or 'No'."
        - "Based on the tweet above, respond with a single word: Yes or No. Is there polarizing rhetoric?"
        - "You are a binary classifier. Classify this tweet as: Yes (polarizing) or No (not polarizing). Output only the classification."
        - "IMPORTANT: Respond with only 'Yes' or 'No'. Does this tweet contain polarizing rhetoric? No explanations needed."
    
    few_shot_learning:
      enabled: true
      n_examples: 5
      include_positive_negative: true
      examples:
        - tweet: "There is no place for white nationalists in our society"
          label: "Yes"
        - tweet: "Slavery will forever be a stain on our country's history"
          label: "No"
        - tweet: "Trump's despicable behavior shows his true character"
          label: "Yes"
        - tweet: "While Florida suffers from gun violence, politicians debate"
          label: "No"
        - tweet: "GOP blames disastrous increase in wildfires on environmental policies"
          label: "Yes"

# Prompt Templates
prompts:
  basic_polarization:
    template: |
      Does this tweet contain polarizing rhetoric?
      
      Tweet: {tweet_text}
      
      Answer:
  
  binary_classification:
    template: |
      Classify this tweet as polarizing or not polarizing. Respond with only 'Yes' or 'No'.
      
      Tweet: {tweet_text}
      
      Response:
  
  few_shot:
    template: |
      Here are some examples of tweets and whether they contain polarizing rhetoric:
      
      {examples}
      
      Now classify this tweet:
      Tweet: {tweet_text}
      Contains polarizing rhetoric:

# Output Configuration
output:
  base_directory: "results"
  save_formats: ["json", "csv"]
  timestamp_files: true
  compress_large_files: false
  
  # File naming patterns
  patterns:
    experiment_results: "experiment_results_{timestamp}"
    task_results: "task_{task_name}_{timestamp}"
    analysis_results: "analysis_{timestamp}"
    visualizations: "plots_{timestamp}"

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(levelname)s - %(message)s"
  log_to_file: true
  log_file: "experiment.log"
  log_to_console: true

# Performance Configuration
performance:
  batch_processing: true
  batch_size: 10
  parallel_processing: false  # Set to true if multiple GPUs available
  memory_optimization: true
  cache_responses: true

# Evaluation Metrics
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "clarity_rate"
    - "response_length"
  
  thresholds:
    min_accuracy: 0.5
    min_clarity_rate: 0.7
    max_response_length: 500

# Visualization Settings
visualization:
  enabled: true
  formats: ["png", "pdf"]
  dpi: 300
  style: "seaborn"
  color_palette: "Set2"
  
  plots:
    - "label_distribution"
    - "party_distribution"
    - "response_length_histogram"
    - "accuracy_by_strategy"
    - "confusion_matrix"

# Advanced Configuration
advanced:
  # Attention mechanism analysis (for future extension)
  attention_analysis:
    enabled: false
    layer_analysis: true
    head_analysis: true
    token_analysis: true
  
  # Model comparison (for future extension)
  model_comparison:
    enabled: false
    models:
      - "gpt2"
      - "amd/AMD-OLMo-1B-SFT-DPO"
      - "microsoft/DialoGPT-medium"
  
  # Custom preprocessing
  preprocessing:
    clean_text: true
    normalize_whitespace: true
    remove_urls: false
    remove_mentions: false
    max_length: 280







# Advanced NLP Project .gitignore
# Author: Peter Chika Ozo-ogueji

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Virtual environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# Data files and datasets
*.csv
*.tsv
*.json
*.jsonl
*.pkl
*.pickle
*.h5
*.hdf5
*.parquet

# Model files and checkpoints
*.pt
*.pth
*.bin
*.safetensors
models/
checkpoints/
cache/
.cache/

# Hugging Face cache
~/.cache/huggingface/

# Results and outputs
results/
outputs/
logs/
plots/
figures/
*.log
*.out
*.err

# Experiment artifacts
experiment_*.json
task_*.json
analysis_*.json
*_results_*.json
*_results_*.csv
experiment_report_*.txt

# API keys and secrets
.env
.env.local
.env.*.local
secrets.json
config_secret.yaml

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store
Thumbs.db

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
# Uncomment if you want to ignore specific files
# affective_polarization_train.csv
# sampled_tweets.csv
# tweets_prompts.txt
# generated_responses.json
# binary_extraction_chats.json
# binary_extraction_results.json
# improved_prompt_results.json
# few_shot_learning_results.json

# Temporary files
tmp/
temp/
*.tmp
*.temp

# Documentation build
docs/_build/
docs/build/
site/

# PyCharm
.idea/

# Spyder
.spyderproject
.spyproject

# Rope
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# Large files (use Git LFS if needed)
*.zip
*.tar.gz
*.rar
*.7z

# Weights and large model files
*.weight
*.weights
*.model

# Backup files
*.bak
*.backup
*.orig

# Local configuration overrides
config_local.yaml
config_override.yaml
